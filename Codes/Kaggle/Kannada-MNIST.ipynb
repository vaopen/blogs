{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "# REF: https://www.kaggle.com/bustam/cnn-in-keras-for-kannada-digits\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "IW = 28\n",
    "IH = 28\n",
    "TP = 10\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        \n",
    "        root = '/kaggle/input/Kannada-MNIST/'\n",
    "        sample_file = os.path.join(root, 'sample_submission.csv')\n",
    "        dev_hard_file = os.path.join(root, 'Dig-MNIST.csv')\n",
    "        train_file = os.path.join(root, 'train.csv')\n",
    "        test_file = os.path.join(root, 'test.csv')\n",
    "\n",
    "        for file in [dev_hard_file, sample_file, train_file, test_file]:\n",
    "            assert os.path.exists(file), 'Please download dataset and save to \"data/Kannada-MNIST/\" before boot'\n",
    "\n",
    "        X_train_all, Y_train_all = self.read_csv(train_file, type='train')\n",
    "        train_pers, dev_pers = self.random_divide(len(X_train_all), props=[0.95, 0.05])\n",
    "        self.X_train, self.Y_train = X_train_all[train_pers], Y_train_all[train_pers]\n",
    "        self.X_dev, self.Y_dev = X_train_all[dev_pers], Y_train_all[dev_pers]\n",
    "        self.X_dev_hard, self.Y_dev_hard = self.read_csv(dev_hard_file, type='dev_hard')\n",
    "        self.X_test, _ = self.read_csv(test_file, type='test')\n",
    "        self.num_train_data, self.num_dev_data, self.num_test_data = self.X_train.shape[0], self.X_dev.shape[0], self.X_test.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv(file_path, type='train'):\n",
    "        \"\"\" 读取 csv 数据 \"\"\"\n",
    "        \n",
    "        data = pd.read_csv(file_path)\n",
    "        X = data.iloc[: ,1: ].values\n",
    "        Y = data.iloc[: ,0].values \n",
    "        \n",
    "        X = X.reshape((X.shape[0], IH, IW, 1))\n",
    "        Y = np.eye(TP)[Y] if type != 'test' else None\n",
    "        return X, Y\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_divide(data_size, props):\n",
    "        \"\"\" 返回将数据按 props 切分后的索引 \"\"\"\n",
    "        \n",
    "        assert sum(props) == 1\n",
    "        permutation = list(np.random.permutation(data_size))\n",
    "        return [permutation[int(data_size * sum(props[: i])): int(data_size * sum(props[: i+1]))] for i in range(len(props))]\n",
    "\n",
    "    def random_mini_batches(self, batch_size = 64):\n",
    "        \"\"\" 切分训练集为 mini_batch \"\"\"\n",
    "        \n",
    "        data_size = len(self.X_train)\n",
    "        permutation = list(np.random.permutation(data_size))\n",
    "        batch_permutation_indices = [permutation[i: i + batch_size] for i in range(0, data_size, batch_size)]\n",
    "        for batch_permutation in batch_permutation_indices:\n",
    "            yield self.X_train[batch_permutation], self.Y_train[batch_permutation]\n",
    "            \n",
    "data_loader = MNISTLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    \"\"\" CNN 模型 \"\"\"\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        #         self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        #         x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense3(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(tf.keras.Model):\n",
    "    \"\"\" Xception 模型 \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.separable_conv2 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.separable_conv3 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.separable_conv4 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.separable_conv5 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.separable_conv6 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool6 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.separable_conv7 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.separable_conv8 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.separable_conv9 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool9 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.separable_conv10 = tf.keras.layers.SeparableConv2D(\n",
    "            filters=10,\n",
    "            kernel_size=[3, 3],\n",
    "            padding='same'\n",
    "        )\n",
    "        \n",
    "        #         self.dw_conv = tf.keras.layers.DepthwiseConv2D(\n",
    "        #             kernel_size=(7, 7),\n",
    "        #             strides=(1, 1),\n",
    "        #             padding='valid',\n",
    "        #         )\n",
    "        self.global_average_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        #         self.dense = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.separable_conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.separable_conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        res = x\n",
    "        x = self.separable_conv4(x)\n",
    "        x = self.separable_conv5(x)\n",
    "        x = self.separable_conv6(x)\n",
    "        x += res\n",
    "        res = x\n",
    "        x = self.separable_conv7(x)\n",
    "        x = self.separable_conv8(x)\n",
    "        x = self.separable_conv9(x)\n",
    "        x += res\n",
    "        x = self.pool9(x)\n",
    "        x = self.separable_conv10(x)\n",
    "        #         x = self.dw_conv(x)\n",
    "        x = self.global_average_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        #         x = self.dense(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        assert units % 32 == 0\n",
    "        self.k = units // 32\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        k = self.k\n",
    "        self.conv_1_1 = tf.keras.layers.Conv2D(\n",
    "            filters=8*k,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv_1_1_t3 = tf.keras.layers.Conv2D(\n",
    "            filters=12*k,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv_3_3 = tf.keras.layers.Conv2D(\n",
    "            filters=16*k,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv_1_1_t5 = tf.keras.layers.Conv2D(\n",
    "            filters=k,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv_3_3_t5 = tf.keras.layers.Conv2D(\n",
    "            filters=2*k,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv_5_5 = tf.keras.layers.Conv2D(\n",
    "            filters=4*k,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool_t = tf.keras.layers.MaxPool2D(\n",
    "            pool_size=[3, 3],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.pool = tf.keras.layers.Conv2D(\n",
    "            filters=4*k,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_1_1 = self.conv_1_1(inputs)\n",
    "        x_1_1_t3 = self.conv_1_1_t3(inputs)\n",
    "        x_3_3 = self.conv_3_3(x_1_1_t3)\n",
    "        x_1_1_t5 = self.conv_1_1_t5(inputs)\n",
    "        x_3_3_t5 = self.conv_3_3_t5(x_1_1_t5)\n",
    "        x_5_5 = self.conv_5_5(x_3_3_t5)\n",
    "        x_pool_t = self.pool_t(inputs)\n",
    "        x_pool = self.pool(x_pool_t)\n",
    "        x = tf.concat([x_1_1, x_3_3, x_5_5, x_pool], axis=-1)\n",
    "        x = self.bn(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Inception(tf.keras.Model):\n",
    "    \"\"\" Inception 模型 \"\"\"\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layer1_1 = InceptionModule(units=32)\n",
    "        self.layer1_2 = InceptionModule(units=32)\n",
    "        self.layer1_3 = InceptionModule(units=64, activation=tf.nn.relu)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.layer2_1 = InceptionModule(units=64)\n",
    "        self.layer2_2 = InceptionModule(units=64)\n",
    "        self.layer2_3 = InceptionModule(units=64, activation=tf.nn.relu)\n",
    "        self.layer3_1 = InceptionModule(units=64)\n",
    "        self.layer3_2 = InceptionModule(units=64)\n",
    "        self.layer3_3 = InceptionModule(units=64, activation=tf.nn.relu)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.layer4_1 = InceptionModule(units=128)\n",
    "        self.layer4_2 = InceptionModule(units=128)\n",
    "        self.layer4_3 = InceptionModule(units=64, activation=tf.nn.relu)\n",
    "        self.pool4 = tf.keras.layers.MaxPool2D(pool_size=[3, 3], strides=2)\n",
    "        self.conv5 = tf.keras.layers.Conv2D(\n",
    "            filters=10,\n",
    "            kernel_size=[1, 1],\n",
    "            padding='same'\n",
    "        )\n",
    "        self.global_average_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.layer1_1(x)\n",
    "        x = self.layer1_2(x)\n",
    "        x = self.layer1_3(x)\n",
    "        x = self.pool1(x)\n",
    "        res = x\n",
    "        x = self.layer2_1(x)\n",
    "        x = self.layer2_2(x)\n",
    "        x = self.layer2_3(x)\n",
    "        x += res\n",
    "        res = x\n",
    "        x = self.layer3_1(x)\n",
    "        x = self.layer3_2(x)\n",
    "        x = self.layer3_3(x)\n",
    "        x += res\n",
    "        x = self.pool2(x)\n",
    "        res = x\n",
    "        x = self.layer3_1(x)\n",
    "        x = self.layer3_2(x)\n",
    "        x = self.layer3_3(x)\n",
    "        x += res\n",
    "        res = x\n",
    "        x = self.layer4_1(x)\n",
    "        x = self.layer4_2(x)\n",
    "        x = self.layer4_3(x)\n",
    "        x += res\n",
    "        x = self.pool4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.global_average_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 50\n",
    "print_step = 1000\n",
    "dev_step = 100000\n",
    "learning_rate = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(dropout_rate=0.5)\n",
    "# model = Xception()\n",
    "model = Inception()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hots_to_labels(one_hots):\n",
    "    return np.array([[np.argmax(one_hot)] for one_hot in one_hots])\n",
    "\n",
    "def predict(model, data_loader, batch_size=10000, type='train'):\n",
    "    wrongs = 0\n",
    "    data_length = data_loader.num_train_data if type == 'train' else data_loader.num_dev_data\n",
    "    X_map = {\"train\": data_loader.X_train, \"dev\": data_loader.X_dev, \"dev_hard\": data_loader.X_dev_hard}\n",
    "    Y_map = {\"train\": data_loader.Y_train, \"dev\": data_loader.Y_dev, \"dev_hard\": data_loader.Y_dev_hard}\n",
    "    X = X_map[type]\n",
    "    Y = Y_map[type]\n",
    "    for i in range(0, data_length, batch_size):\n",
    "        print(f'predict {i}', end='\\r')\n",
    "        X_batch = X[i: i + batch_size] / 255.\n",
    "        Y_batch = Y[i: i + batch_size]\n",
    "        Y_batch_= model.predict(X_batch)\n",
    "        Y_batch, Y_batch_ = one_hots_to_labels(Y_batch), one_hots_to_labels(Y_batch_)\n",
    "        mask = Y_batch.reshape((Y_batch.shape[0], )) - Y_batch_.reshape(Y_batch_.shape[0], )\n",
    "        wrongs += len(np.flatnonzero(mask))\n",
    "    return 1- wrongs / data_length\n",
    "\n",
    "print(predict(model, data_loader, type='dev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 10,\n",
    "                                   width_shift_range = 0.25,\n",
    "                                   height_shift_range = 0.25,\n",
    "                                   shear_range = 0.1,\n",
    "                                   zoom_range = 0.25,\n",
    "                                   horizontal_flip = False)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        Y_ = model(X)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\n",
    "            y_true=Y,\n",
    "            y_pred=Y_\n",
    "        ))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "# for i in range(2):\n",
    "    num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "    for j, (X, Y) in enumerate(train_datagen.flow(data_loader.X_train, data_loader.Y_train, batch_size=batch_size)):\n",
    "        loss = train_one_step(X, Y)\n",
    "        if (i * data_loader.num_train_data + j * batch_size) % print_step == 0:\n",
    "            print(f\"{i} - {j * batch_size: 6}: loss {loss.numpy()}\")\n",
    "        if (i * data_loader.num_train_data + j * batch_size) % dev_step == 0:\n",
    "            train_accuracy = predict(model, data_loader, type='train')\n",
    "            dev_accuracy = predict(model, data_loader, type='dev')\n",
    "            dev_hard_accuracy = predict(model, data_loader, type='dev_hard')\n",
    "            print(f'train accuracy: {train_accuracy: .2%} dev accuracy: {dev_accuracy: .2%} dev hard accuracy: {dev_hard_accuracy: .2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, data_loader, type='train'))\n",
    "print(predict(model, data_loader, type='dev'))\n",
    "print(predict(model, data_loader, type='dev_hard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_csv(file_path, labels):\n",
    "#     csv_list = [['id', 'label']]\n",
    "#     for i, label in enumerate(labels):\n",
    "#         csv_list.append([str(i), str(label)])\n",
    "#     csv_str = ''\n",
    "#     for line_list in csv_list:\n",
    "#         csv_str += ','.join(line_list) + '\\n'\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         f.write(csv_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_ = one_hots_to_labels(model.predict(data_loader.X_test / 255.))\n",
    "labels = Y_test_.reshape((Y_test_.shape[0], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_csv('/kaggle/working/submission.csv', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/Kannada-MNIST/sample_submission.csv')\n",
    "submission['label'] = labels\n",
    "submission.head()\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
